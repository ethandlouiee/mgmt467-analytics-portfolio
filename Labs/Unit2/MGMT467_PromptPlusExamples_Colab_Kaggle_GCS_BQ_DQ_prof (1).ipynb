{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFGDrMH6oASh"
      },
      "source": [
        "# MGMT 467 — Prompt-Driven Lab (with Commented Examples)\n",
        "## Kaggle ➜ Google Cloud Storage ➜ BigQuery ➜ Data Quality (DQ)\n",
        "\n",
        "**How to use this notebook**\n",
        "- Each section gives you a **Build Prompt** to paste into Gemini/Vertex AI (or Gemini in Colab).\n",
        "- Below each prompt, you’ll see a **commented example** of what a good LLM answer might look like.\n",
        "- **Do not** just uncomment and run. Use the prompt to generate your own code, then compare to the example.\n",
        "- After every step, run the **Verification Prompt**, and write the **Reflection** in Markdown.\n",
        "\n",
        "> Goal today: Download the Netflix dataset (Kaggle) → Stage on GCS → Load into BigQuery → Run DQ profiling (missingness, duplicates, outliers, anomaly flags).\n"
      ],
      "id": "GFGDrMH6oASh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZFI8W0joASi"
      },
      "source": [
        "### Academic integrity & LLM usage\n",
        "- Use the prompts here to generate your own code cells.\n",
        "- Read concept notes and write the reflection answers in your own words.\n",
        "- Keep credentials out of code. Upload `kaggle.json` when asked.\n"
      ],
      "id": "FZFI8W0joASi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwpFpFQYoASj"
      },
      "source": [
        "## Learning objectives\n",
        "1) Explain **why** we stage data in GCS and load it to BigQuery.  \n",
        "2) Build an **idempotent**, auditable pipeline.  \n",
        "3) Diagnose **missingness**, **duplicates**, and **outliers** and justify cleaning choices.  \n",
        "4) Connect DQ decisions to **business/ML impact**.\n"
      ],
      "id": "vwpFpFQYoASj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYFZvZKhoASj"
      },
      "source": [
        "## 0) Environment setup — What & Why\n",
        "Authenticate Colab to Google Cloud so we can use `gcloud`, GCS, and BigQuery. Set **PROJECT_ID** and **REGION** once for consistency (cost/latency)."
      ],
      "id": "QYFZvZKhoASj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd75y1hAoASj"
      },
      "source": [
        "### Build Prompt (paste to LLM)\n",
        "You are my cloud TA. Generate a single **Colab code cell** that:\n",
        "1) Authenticates to Google Cloud in Colab,  \n",
        "2) Prompts for `PROJECT_ID` via `input()` and sets `REGION=\"us-central1\"` (editable),  \n",
        "3) Exports `GOOGLE_CLOUD_PROJECT`,  \n",
        "4) Runs `gcloud config set project $GOOGLE_CLOUD_PROJECT`,  \n",
        "5) Prints both values. Add 2–3 comments explaining what/why.\n",
        "End with a comment: `# Done: Auth + Project/Region set`.\n"
      ],
      "id": "Fd75y1hAoASj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9c21328",
        "outputId": "36892665-6424-47e7-e784-0a5a33a5e395"
      },
      "source": [
        "from google.colab import auth\n",
        "import os\n",
        "\n",
        "# Authenticate to Google Cloud.\n",
        "# This allows your Colab notebook to access Google Cloud services.\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Prompt for the Project ID and set the region.\n",
        "# Using input() makes the notebook reusable without hardcoding credentials.\n",
        "PROJECT_ID = input(\"Enter your GCP Project ID: \").strip()\n",
        "REGION = \"us-central1\"  # You can change this to your preferred region.\n",
        "\n",
        "# Set the project ID as an environment variable.\n",
        "# This is a standard way for tools like the gcloud CLI to find the active project.\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "\n",
        "# Configure the gcloud command-line tool to use your project.\n",
        "!gcloud config set project $GOOGLE_CLOUD_PROJECT\n",
        "\n",
        "print(f\"Project ID: {PROJECT_ID}\")\n",
        "print(f\"Region: {REGION}\")\n",
        "\n",
        "# Done: Auth + Project/Region set"
      ],
      "id": "c9c21328",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: google.colab.auth.authenticate_user() is not supported in Colab Enterprise.\n",
            "Enter your GCP Project ID: noble-broker-471012-q6\n",
            "Updated property [core/project].\n",
            "Project ID: noble-broker-471012-q6\n",
            "Region: us-central1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEIykdfioASk"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a short cell that prints the active project using `gcloud config get-value project` and echoes the `REGION` you set.\n"
      ],
      "id": "BEIykdfioASk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification step: Check active project and region\n",
        "!gcloud config get-value project\n",
        "import os\n",
        "print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKQVtt4cSqUn",
        "outputId": "c60c8147-8e9e-4f30-d183-215618c47dc2"
      },
      "id": "gKQVtt4cSqUn",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "noble-broker-471012-q6\n",
            "Project: noble-broker-471012-q6 | Region: us-central1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOb_eekKoASk"
      },
      "source": [
        "**Reflection:** Why do we set `PROJECT_ID` and `REGION` at the top? What can go wrong if we don’t?"
      ],
      "id": "TOb_eekKoASk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gf8c7gpoASk"
      },
      "source": [
        "## 1) Kaggle API — What & Why\n",
        "Use Kaggle CLI for reproducible downloads. Store `kaggle.json` at `~/.kaggle/kaggle.json` with `0600` permissions to protect secrets."
      ],
      "id": "3Gf8c7gpoASk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjRY9WSFoASk"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **single Colab code cell** that:\n",
        "- Prompts me to upload `kaggle.json`,\n",
        "- Saves to `~/.kaggle/kaggle.json` with `0600` permissions,\n",
        "- Prints `kaggle --version`.\n",
        "Add comments about security and reproducibility.\n"
      ],
      "id": "MjRY9WSFoASk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "7ebd8186",
        "outputId": "6b94f0d8-3a34-40b8-becb-ec937310de1e"
      },
      "source": [
        "# EXAMPLE (from LLM) — Kaggle setup (commented)\n",
        "from google.colab import files\n",
        "print(\"Upload your kaggle.json (Kaggle > Account > Create New API Token)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
        "    f.write(uploaded[list(uploaded.keys())[0]])\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)  # owner-only\n",
        "\n",
        "!kaggle --version"
      ],
      "id": "7ebd8186",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your kaggle.json (Kaggle > Account > Create New API Token)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-10cfe614-ce9e-49f6-8af0-7af2394d4c02\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-10cfe614-ce9e-49f6-8af0-7af2394d4c02\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Kaggle API 1.7.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thMOKW2ZoASk"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a one-liner that runs `kaggle --help | head -n 20` to show the CLI is ready.\n"
      ],
      "id": "thMOKW2ZoASk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB2Vy-teoASk"
      },
      "source": [
        "**Reflection:** Why require strict `0600` permissions on API tokens? What risks are we avoiding?"
      ],
      "id": "pB2Vy-teoASk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11850c0c",
        "outputId": "87a20602-e209-4501-9de8-64e4d41edf96"
      },
      "source": [
        "!kaggle --help | head -n 20"
      ],
      "id": "11850c0c",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: kaggle [-h] [-v] [-W]\n",
            "              {competitions,c,datasets,d,kernels,k,models,m,files,f,config}\n",
            "              ...\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  -v, --version         Print the Kaggle API version\n",
            "  -W, --no-warn         Disable out-of-date API version warning\n",
            "\n",
            "commands:\n",
            "  {competitions,c,datasets,d,kernels,k,models,m,files,f,config}\n",
            "                        Use one of:\n",
            "                        competitions {list, files, download, submit, submissions, leaderboard}\n",
            "                        datasets {list, files, download, create, version, init, metadata, status}\n",
            "                        kernels {list, files, init, push, pull, output, status}\n",
            "                        models {instances, get, list, init, create, delete, update}\n",
            "                        models instances {versions, get, files, init, create, delete, update}\n",
            "                        models instances versions {init, create, download, delete, files}\n",
            "                        config {view, set, unset}\n",
            "    competitions (c)    Commands related to Kaggle competitions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7fac262"
      },
      "source": [
        "Strict `0600` permissions are required for API tokens and other credentials as a critical security measure. This permission setting ensures that only the owner of the file (in this case, your user account in the Colab environment) can read and write the file.\n",
        "\n",
        "**Risks we are avoiding:**\n",
        "1.  **Unauthorized Access:** If other users or processes on the system had read access, they could steal your API token.\n",
        "2.  **Impersonation:** A stolen token could be used to act on your behalf, potentially downloading datasets you didn't intend or participating in competitions under your name.\n",
        "3.  **Resource Misuse:** An attacker could use your credentials to exhaust API rate limits or perform other malicious actions tied to your account.\n",
        "\n",
        "By enforcing `0600` permissions, the Kaggle CLI client helps prevent accidental or malicious exposure of your secret credentials."
      ],
      "id": "c7fac262"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD9kJSE4oASl"
      },
      "source": [
        "## 2) Download & unzip dataset — What & Why\n",
        "Keep raw files under `/content/data/raw` for predictable paths and auditing.\n",
        "**Dataset:** `sayeeduddin/netflix-2025user-behavior-dataset-210k-records`"
      ],
      "id": "BD9kJSE4oASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRhSvGi4oASl"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates `/content/data/raw`,\n",
        "- Downloads the dataset to `/content/data` with Kaggle CLI,\n",
        "- Unzips into `/content/data/raw` (overwrite OK),\n",
        "- Lists all CSVs with sizes in a neat table.\n",
        "Include comments describing each step.\n"
      ],
      "id": "vRhSvGi4oASl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the directory to store raw data\n",
        "!mkdir -p /content/data/raw\n",
        "\n",
        "# Download the dataset using Kaggle CLI to /content/data\n",
        "# The -d flag specifies the dataset, and -p specifies the download path\n",
        "!kaggle datasets download -d sayeeduddin/netflix-2025user-behavior-dataset-210k-records -p /content/data\n",
        "\n",
        "# Unzip the downloaded dataset into the raw data directory\n",
        "# -o flag overwrites files if they exist\n",
        "!unzip -o /content/data/*.zip -d /content/data/raw\n",
        "\n",
        "# List all CSV files in the raw data directory with their sizes in a neat table\n",
        "!ls -lh /content/data/raw/*.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8_U7PESet1j",
        "outputId": "39ce191b-c567-4b9a-a678-0af7886cbba2"
      },
      "id": "L8_U7PESet1j",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/sayeeduddin/netflix-2025user-behavior-dataset-210k-records\n",
            "License(s): CC0-1.0\n",
            "Downloading netflix-2025user-behavior-dataset-210k-records.zip to /content/data\n",
            "  0% 0.00/4.02M [00:00<?, ?B/s]\n",
            "100% 4.02M/4.02M [00:00<00:00, 519MB/s]\n",
            "Archive:  /content/data/netflix-2025user-behavior-dataset-210k-records.zip\n",
            "  inflating: /content/data/raw/README.md  \n",
            "  inflating: /content/data/raw/movies.csv  \n",
            "  inflating: /content/data/raw/recommendation_logs.csv  \n",
            "  inflating: /content/data/raw/reviews.csv  \n",
            "  inflating: /content/data/raw/search_logs.csv  \n",
            "  inflating: /content/data/raw/users.csv  \n",
            "  inflating: /content/data/raw/watch_history.csv  \n",
            "-rw-r--r-- 1 root root 114K Aug  2 19:36 /content/data/raw/movies.csv\n",
            "-rw-r--r-- 1 root root 4.5M Aug  2 19:36 /content/data/raw/recommendation_logs.csv\n",
            "-rw-r--r-- 1 root root 1.8M Aug  2 19:36 /content/data/raw/reviews.csv\n",
            "-rw-r--r-- 1 root root 2.2M Aug  2 19:36 /content/data/raw/search_logs.csv\n",
            "-rw-r--r-- 1 root root 1.6M Aug  2 19:36 /content/data/raw/users.csv\n",
            "-rw-r--r-- 1 root root 8.9M Aug  2 19:36 /content/data/raw/watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFvFr29hoASl"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that asserts there are exactly **six** CSV files and prints their names.\n"
      ],
      "id": "hFvFr29hoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cd2c7aa",
        "outputId": "552c3ada-50df-4717-f612-87e013739d87"
      },
      "source": [
        "import glob\n",
        "\n",
        "# Find all CSV files in the raw data directory\n",
        "csv_files = glob.glob('/content/data/raw/*.csv')\n",
        "\n",
        "# Assert that there are exactly six CSV files\n",
        "assert len(csv_files) == 6, f\"Expected 6 CSV files, but found {len(csv_files)}\"\n",
        "\n",
        "# Print the names of the files\n",
        "print(\"Found 6 CSV files as expected:\")\n",
        "for f in sorted(csv_files):\n",
        "    print(f.split('/')[-1])"
      ],
      "id": "2cd2c7aa",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6 CSV files as expected:\n",
            "movies.csv\n",
            "recommendation_logs.csv\n",
            "reviews.csv\n",
            "search_logs.csv\n",
            "users.csv\n",
            "watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "they2Q5foASl"
      },
      "source": [
        "**Reflection:** Why is keeping a clean file inventory (names, sizes) useful downstream?"
      ],
      "id": "they2Q5foASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "437aaff3"
      },
      "source": [
        "Keeping a clean file inventory (names, sizes) is useful for several reasons:\n",
        "\n",
        "*   **Auditing and Verification:** It allows you to quickly verify that the download and unzip processes completed successfully and that you have the expected number of files with reasonable sizes. Any significant deviation can signal a problem early.\n",
        "*   **Debugging:** If a downstream process fails (e.g., a BigQuery load job), the file inventory is the first place to check. You can confirm the file exists, its name is correct, and its size isn't zero.\n",
        "*   **Resource Planning:** Knowing file sizes helps in estimating the resources required for subsequent steps, such as GCS storage costs, BigQuery ingest costs, and the memory needed to process the data."
      ],
      "id": "437aaff3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5PQ-P1yoASl"
      },
      "source": [
        "## 3) Create GCS bucket & upload — What & Why\n",
        "Stage in GCS → consistent, versionable source for BigQuery loads. Bucket names must be **globally unique**."
      ],
      "id": "X5PQ-P1yoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_IZooOUoASl"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates a unique bucket in `${REGION}` (random suffix),\n",
        "- Saves name to `BUCKET_NAME` env var,\n",
        "- Uploads all CSVs to `gs://$BUCKET_NAME/netflix/`,\n",
        "- Prints the bucket name and explains staging benefits.\n"
      ],
      "id": "b_IZooOUoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3xtQmu5oASl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "782c7f93-47cd-4cc1-bcb6-64dc172f8c59"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating bucket: gs://mgmt467-netflix-3fc7b4d1 in region: us-central1\n",
            "Creating gs://mgmt467-netflix-3fc7b4d1/...\n",
            "\n",
            "Uploading files to gs://mgmt467-netflix-3fc7b4d1/netflix/\n",
            "Copying file:///content/data/raw/movies.csv to gs://mgmt467-netflix-3fc7b4d1/netflix/movies.csv\n",
            "Copying file:///content/data/raw/recommendation_logs.csv to gs://mgmt467-netflix-3fc7b4d1/netflix/recommendation_logs.csv\n",
            "Copying file:///content/data/raw/reviews.csv to gs://mgmt467-netflix-3fc7b4d1/netflix/reviews.csv\n",
            "Copying file:///content/data/raw/search_logs.csv to gs://mgmt467-netflix-3fc7b4d1/netflix/search_logs.csv\n",
            "Copying file:///content/data/raw/users.csv to gs://mgmt467-netflix-3fc7b4d1/netflix/users.csv\n",
            "Copying file:///content/data/raw/watch_history.csv to gs://mgmt467-netflix-3fc7b4d1/netflix/watch_history.csv\n",
            "\n",
            "Average throughput: 171.6MiB/s\n",
            "\n",
            "Successfully created bucket: mgmt467-netflix-3fc7b4d1 and uploaded files.\n",
            "\n",
            "Benefits of staging data in GCS:\n",
            "- **Decoupling & Reusability:** GCS acts as a stable, centralized source of truth. Multiple services (like BigQuery, Dataproc, or Vertex AI) can read from the same data without interfering with each other.\n",
            "- **Durability & Availability:** GCS is a highly durable and available service, ensuring your raw data is not lost if your Colab runtime disconnects.\n",
            "- **Efficient Loading:** Loading data into BigQuery from GCS is much faster and more reliable than loading from a local machine.\n",
            "\n",
            "Verifying contents of gs://mgmt467-netflix-3fc7b4d1/netflix/\n",
            "gs://mgmt467-netflix-3fc7b4d1/:\n",
            "\n",
            "gs://mgmt467-netflix-3fc7b4d1/netflix/:\n",
            "gs://mgmt467-netflix-3fc7b4d1/netflix/movies.csv\n",
            "gs://mgmt467-netflix-3fc7b4d1/netflix/recommendation_logs.csv\n",
            "gs://mgmt467-netflix-3fc7b4d1/netflix/reviews.csv\n",
            "gs://mgmt467-netflix-3fc7b4d1/netflix/search_logs.csv\n",
            "gs://mgmt467-netflix-3fc7b4d1/netflix/users.csv\n",
            "gs://mgmt467-netflix-3fc7b4d1/netflix/watch_history.csv\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "import os\n",
        "\n",
        "# The REGION variable was defined in a previous cell.\n",
        "# We will generate a unique bucket name to avoid conflicts.\n",
        "BUCKET_NAME = f\"mgmt467-netflix-{uuid.uuid4().hex[:8]}\"\n",
        "os.environ[\"BUCKET_NAME\"] = BUCKET_NAME\n",
        "\n",
        "print(f\"Creating bucket: gs://{BUCKET_NAME} in region: {REGION}\")\n",
        "# Use the REGION variable directly in the gcloud command.\n",
        "!gcloud storage buckets create gs://$BUCKET_NAME --location=$REGION\n",
        "\n",
        "# Upload all CSVs from the raw data directory to a 'netflix' prefix in the bucket.\n",
        "print(f\"\\nUploading files to gs://{BUCKET_NAME}/netflix/\")\n",
        "!gcloud storage cp /content/data/raw/*.csv gs://$BUCKET_NAME/netflix/\n",
        "\n",
        "# Staging data in GCS is a best practice for several reasons.\n",
        "print(f\"\\nSuccessfully created bucket: {BUCKET_NAME} and uploaded files.\")\n",
        "print(\"\\nBenefits of staging data in GCS:\")\n",
        "print(\"- **Decoupling & Reusability:** GCS acts as a stable, centralized source of truth. Multiple services (like BigQuery, Dataproc, or Vertex AI) can read from the same data without interfering with each other.\")\n",
        "print(\"- **Durability & Availability:** GCS is a highly durable and available service, ensuring your raw data is not lost if your Colab runtime disconnects.\")\n",
        "print(\"- **Efficient Loading:** Loading data into BigQuery from GCS is much faster and more reliable than loading from a local machine.\")\n",
        "\n",
        "# Verify the contents of the bucket to ensure the upload was successful.\n",
        "print(f\"\\nVerifying contents of gs://{BUCKET_NAME}/netflix/\")\n",
        "!gcloud storage ls --recursive gs://$BUCKET_NAME"
      ],
      "id": "O3xtQmu5oASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZhif_knoASl"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that lists the `netflix/` prefix and shows object sizes.\n"
      ],
      "id": "kZhif_knoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "128e22af",
        "outputId": "c08781e2-33bd-415e-b3d5-21c007de693b"
      },
      "source": [
        "# List the files in the GCS bucket prefix with human-readable sizes.\n",
        "!gcloud storage ls --readable-sizes gs://$BUCKET_NAME/netflix/"
      ],
      "id": "128e22af",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://mgmt467-netflix-3fc7b4d1/netflix/movies.csv\n",
            "gs://mgmt467-netflix-3fc7b4d1/netflix/recommendation_logs.csv\n",
            "gs://mgmt467-netflix-3fc7b4d1/netflix/reviews.csv\n",
            "gs://mgmt467-netflix-3fc7b4d1/netflix/search_logs.csv\n",
            "gs://mgmt467-netflix-3fc7b4d1/netflix/users.csv\n",
            "gs://mgmt467-netflix-3fc7b4d1/netflix/watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwgInaDFoASl"
      },
      "source": [
        "**Reflection:** Name two benefits of staging in GCS vs loading directly from local Colab."
      ],
      "id": "CwgInaDFoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28882279"
      },
      "source": [
        "Staging data in Google Cloud Storage (GCS) before loading it into BigQuery offers two key advantages over loading directly from a local Colab environment:\n",
        "\n",
        "1.  **Durability and Reliability:** GCS is a persistent and highly available storage service. If your Colab notebook disconnects or the runtime is recycled, your local data is lost. By staging the data in GCS, you create a stable, durable source of truth that is decoupled from your Colab session, allowing for reliable and repeatable data loading.\n",
        "\n",
        "2.  **Performance and Speed:** Loading data into BigQuery from GCS is significantly faster than uploading from a local machine. The transfer occurs over Google's high-speed internal network, bypassing potential bottlenecks like your local internet connection. This makes the ingestion process much more efficient, especially for large datasets."
      ],
      "id": "28882279"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7ApPnyBoASl"
      },
      "source": [
        "## 4) BigQuery dataset & loads — What & Why\n",
        "Create dataset `netflix` and load six CSVs with **autodetect** for speed (we’ll enforce schemas later)."
      ],
      "id": "k7ApPnyBoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mvCSfFhoASl"
      },
      "source": [
        "### Build Prompt (two cells)\n",
        "**Cell A:** Create (idempotently) dataset `netflix` in US multi-region; if it exists, print a friendly message.  \n",
        "**Cell B:** Load tables from `gs://$BUCKET_NAME/netflix/`:\n",
        "`users, movies, watch_history, recommendation_logs, search_logs, reviews`\n",
        "with `--skip_leading_rows=1 --autodetect --source_format=CSV`.\n",
        "Finish with row-count queries for each table.\n"
      ],
      "id": "3mvCSfFhoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b428e33b",
        "outputId": "8130f31c-35a2-4048-ef31-69892fab7d24"
      },
      "source": [
        "# Cell A: Create (idempotently) dataset `netflix` in US multi-region\n",
        "DATASET=\"netflix\"\n",
        "# The `|| echo` part makes this command idempotent.\n",
        "# If `bq mk` fails (e.g., dataset exists), it will print a message instead of an error.\n",
        "!bq --location=US mk -d --description \"MGMT467 Netflix dataset\" $DATASET || echo \"Dataset '$DATASET' may already exist.\""
      ],
      "id": "b428e33b",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BigQuery error in mk operation: Dataset 'noble-broker-471012-q6:netflix' already\n",
            "exists.\n",
            "Dataset '' may already exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c46eacf8",
        "outputId": "7ec4d3b5-2081-4bfc-9e70-d023c567e81e"
      },
      "source": [
        "# Cell B: Load tables from GCS and verify counts\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# A dictionary mapping the table names to their corresponding CSV files.\n",
        "tables = {\n",
        "  \"users\": \"users.csv\",\n",
        "  \"movies\": \"movies.csv\",\n",
        "  \"watch_history\": \"watch_history.csv\",\n",
        "  \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "  \"search_logs\": \"search_logs.csv\",\n",
        "  \"reviews\": \"reviews.csv\",\n",
        "}\n",
        "\n",
        "# Retrieve configuration variables set in previous cells.\n",
        "DATASET = \"netflix\"\n",
        "BUCKET_NAME = os.environ.get('BUCKET_NAME')\n",
        "PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT')\n",
        "\n",
        "# Loop through the dictionary to load each CSV file into a BigQuery table.\n",
        "for table_name, file_name in tables.items():\n",
        "  source_uri = f\"gs://{BUCKET_NAME}/netflix/{file_name}\"\n",
        "  table_ref = f\"{PROJECT_ID}:{DATASET}.{table_name}\"\n",
        "  print(f\"Loading {table_name} from {source_uri}...\")\n",
        "  # The bq load command ingests data from GCS.\n",
        "  # --skip_leading_rows=1 tells BQ to ignore the header row in the CSV.\n",
        "  # --autodetect lets BQ automatically infer the table schema from the data.\n",
        "  !bq load --project_id={PROJECT_ID} --skip_leading_rows=1 --autodetect --source_format=CSV {table_ref} {source_uri}\n",
        "\n",
        "print(\"\\n--- Verifying Row Counts ---\")\n",
        "\n",
        "# Initialize the BigQuery client.\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# After loading, loop through the tables again to query their row counts.\n",
        "# This method uses the Python client library, which is more robust than shell commands for queries.\n",
        "for table_name in tables.keys():\n",
        "  table_ref_for_query = f\"{PROJECT_ID}.{DATASET}.{table_name}\"\n",
        "\n",
        "  # Construct the query string.\n",
        "  query = f\"SELECT COUNT(*) AS row_count FROM `{table_ref_for_query}`\"\n",
        "\n",
        "  # Execute the query.\n",
        "  query_job = client.query(query)\n",
        "\n",
        "  # Fetch the result, which will be an iterator with one row.\n",
        "  for row in query_job.result():\n",
        "      print(f\"Table: {table_name:<25} Row Count: {row.row_count}\")"
      ],
      "id": "c46eacf8",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading users from gs://mgmt467-netflix-3fc7b4d1/netflix/users.csv...\n",
            "Waiting on bqjob_raec275f360d3d_00000199f20c6f64_1 ... (1s) Current status: DONE   \n",
            "Loading movies from gs://mgmt467-netflix-3fc7b4d1/netflix/movies.csv...\n",
            "Waiting on bqjob_r1a0113c8608847a_00000199f20c8221_1 ... (1s) Current status: DONE   \n",
            "Loading watch_history from gs://mgmt467-netflix-3fc7b4d1/netflix/watch_history.csv...\n",
            "Waiting on bqjob_r5a61043e3ee42452_00000199f20c9497_1 ... (2s) Current status: DONE   \n",
            "Loading recommendation_logs from gs://mgmt467-netflix-3fc7b4d1/netflix/recommendation_logs.csv...\n",
            "Waiting on bqjob_r50e9313619a69b5c_00000199f20cab64_1 ... (3s) Current status: DONE   \n",
            "Loading search_logs from gs://mgmt467-netflix-3fc7b4d1/netflix/search_logs.csv...\n",
            "Waiting on bqjob_r842a2efdc331f70_00000199f20cc667_1 ... (2s) Current status: DONE   \n",
            "Loading reviews from gs://mgmt467-netflix-3fc7b4d1/netflix/reviews.csv...\n",
            "Waiting on bqjob_r3ae339917a6dde67_00000199f20cde0e_1 ... (2s) Current status: DONE   \n",
            "\n",
            "--- Verifying Row Counts ---\n",
            "Table: users                     Row Count: 41200\n",
            "Table: movies                    Row Count: 4160\n",
            "Table: watch_history             Row Count: 420000\n",
            "Table: recommendation_logs       Row Count: 208000\n",
            "Table: search_logs               Row Count: 106000\n",
            "Table: reviews                   Row Count: 61800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqEU0_sBoASl"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single query that returns `table_name, row_count` for all six tables in `${GOOGLE_CLOUD_PROJECT}.netflix`.\n"
      ],
      "id": "yqEU0_sBoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80624ebd",
        "outputId": "94fda3ed-e1de-43b2-fbb2-5e8a949fde72"
      },
      "source": [
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# This query uses the `__TABLES__` pseudo-view to get metadata about the tables.\n",
        "# It's more efficient than querying each table individually.\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  table_id AS table_name,\n",
        "  row_count\n",
        "FROM\n",
        "  `{project_id}.netflix.__TABLES__`\n",
        "ORDER BY\n",
        "  table_id;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results in a formatted table\n",
        "print(f\"Row counts for dataset: {project_id}.netflix\")\n",
        "print(\"-\" * 40)\n",
        "for row in results:\n",
        "    print(f\"{row.table_name:<25} | {row.row_count:>10,}\")\n",
        "print(\"-\" * 40)"
      ],
      "id": "80624ebd",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row counts for dataset: noble-broker-471012-q6.netflix\n",
            "----------------------------------------\n",
            "movies                    |      4,160\n",
            "recommendation_logs       |    208,000\n",
            "reviews                   |     61,800\n",
            "search_logs               |    106,000\n",
            "users                     |     41,200\n",
            "watch_history             |    420,000\n",
            "watch_history_dedup       |    100,000\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeM4Ik7moASl"
      },
      "source": [
        "**Reflection:** When is `autodetect` acceptable? When should you enforce explicit schemas and why?"
      ],
      "id": "xeM4Ik7moASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b488abbb"
      },
      "source": [
        "**When is `autodetect` acceptable? When should you enforce explicit schemas and why?**\n",
        "\n",
        "`autodetect` is acceptable for initial exploration, ad-hoc analysis, or one-time loads of well-structured data, just like we did here. It's fast and convenient for getting data into BigQuery quickly to start running queries.\n",
        "\n",
        "However, for production pipelines, you should **always enforce explicit schemas**. Here’s why:\n",
        "\n",
        "1.  **Data Integrity & Consistency:** An explicit schema ensures that data being loaded conforms to the expected data types (e.g., `INTEGER`, `TIMESTAMP`, `STRING`). This prevents data corruption, such as a date field being misinterpreted as a string.\n",
        "2.  **Error Prevention:** If the source data changes unexpectedly (e.g., a column is renamed or its data type changes), the load job will fail immediately. This is a good thing—it alerts you to an upstream problem, preventing bad data from silently entering your warehouse.\n",
        "3.  **Performance & Cost:** Specifying a schema allows BigQuery to optimize storage and queries more effectively from the start. Autodetection has to scan the data to infer types, which can sometimes be less optimal."
      ],
      "id": "b488abbb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TOxNJk_oASl"
      },
      "source": [
        "## 5) Data Quality (DQ) — Concepts we care about\n",
        "- **Missingness** (MCAR/MAR/MNAR). Impute vs drop. Add `is_missing_*` indicators.\n",
        "- **Duplicates** (exact vs near). Double-counted engagement corrupts labels & KPIs.\n",
        "- **Outliers** (IQR). Winsorize/cap vs robust models. Always **flag** and explain.\n",
        "- **Reproducibility**. Prefer `CREATE OR REPLACE` and deterministic keys.\n"
      ],
      "id": "3TOxNJk_oASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nht9dQqOoASl"
      },
      "source": [
        "### 5.1 Missingness (users) — What & Why\n",
        "Measure % missing and check if missingness depends on another variable (MAR) → potential bias & instability."
      ],
      "id": "nht9dQqOoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D2oFpiSoASl"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Total rows and % missing in `region`, `plan_tier`, `age_band` from `users`.\n",
        "2) `% plan_tier missing by region` ordered descending. Add comments on MAR.\n"
      ],
      "id": "4D2oFpiSoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08HEz5PooASl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ce3050-7584-4edc-a15d-1eed7e174218"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((41200, 0.0, 0.0, 11.93), {'n': 0, 'pct_missing_country': 1, 'pct_missing_subscription_plan': 2, 'pct_missing_age': 3})\n"
          ]
        }
      ],
      "source": [
        "# EXAMPLE (from LLM) — Missingness profile (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "-- Users: % missing per column\n",
        "WITH base AS (\n",
        "  SELECT COUNT(*) n,\n",
        "         COUNTIF(country IS NULL) miss_country,\n",
        "         COUNTIF(subscription_plan IS NULL) miss_plan,\n",
        "         COUNTIF(age IS NULL) miss_age\n",
        "  FROM `{project_id}.netflix.users`\n",
        ")\n",
        "SELECT n,\n",
        "       ROUND(100*miss_country/n,2) AS pct_missing_country,\n",
        "       ROUND(100*miss_plan/n,2)   AS pct_missing_subscription_plan,\n",
        "       ROUND(100*miss_age/n,2)    AS pct_missing_age\n",
        "FROM base;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "08HEz5PooASl"
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: % plan_tier missing by region ordered descending. Add comments on MAR.\n",
        "\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "-- Users: % missing plan_tier by region\n",
        "-- This checks for Missing At Random (MAR) if missingness in plan_tier\n",
        "-- depends on the region. If so, imputation or modeling needs to account for this.\n",
        "SELECT\n",
        "  country,\n",
        "  ROUND(100 * SUM(CASE WHEN subscription_plan IS NULL THEN 1 ELSE 0 END) / COUNT(*), 2) AS pct_missing_plan_tier\n",
        "FROM\n",
        "  `{project_id}.netflix.users`\n",
        "GROUP BY\n",
        "  country\n",
        "ORDER BY\n",
        "  pct_missing_plan_tier DESC;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "print(\"Percentage of missing subscription_plan by country:\")\n",
        "print(\"-\" * 50)\n",
        "for row in results:\n",
        "    print(f\"Country: {row.country:<20} | Missing Plan Tier: {row.pct_missing_plan_tier:.2f}%\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa6UoUrGMeeh",
        "outputId": "9e62714e-c3da-43ff-b11c-600398bf1155"
      },
      "id": "pa6UoUrGMeeh",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of missing subscription_plan by country:\n",
            "--------------------------------------------------\n",
            "Country: USA                  | Missing Plan Tier: 0.00%\n",
            "Country: Canada               | Missing Plan Tier: 0.00%\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32519673"
      },
      "source": [
        "%load_ext bigquery_magics"
      ],
      "id": "32519673",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4541fb9f",
        "outputId": "db992d16-0f49-4051-d2b7-e9f4c1b2025f"
      },
      "source": [
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"SELECT h.* FROM `{project_id}.netflix.watch_history` h LIMIT 1\"\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "for row in results:\n",
        "    print(row.keys())"
      ],
      "id": "4541fb9f",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['session_id', 'user_id', 'movie_id', 'watch_date', 'device_type', 'watch_duration_minutes', 'progress_percentage', 'action', 'quality', 'location_country', 'is_download', 'user_rating'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si9Z_OIEoASm"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that prints the three missingness percentages from (1), rounded to two decimals.\n"
      ],
      "id": "Si9Z_OIEoASm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3e17cbb",
        "outputId": "4279fee1-9703-44cf-990d-03ccc3c3e0f6"
      },
      "source": [
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "-- Verification: Print missingness percentages\n",
        "WITH base AS (\n",
        "  SELECT COUNT(*) n,\n",
        "         COUNTIF(country IS NULL) miss_country,\n",
        "         COUNTIF(subscription_plan IS NULL) miss_plan,\n",
        "         COUNTIF(age IS NULL) miss_age\n",
        "  FROM `{project_id}.netflix.users`\n",
        ")\n",
        "SELECT ROUND(100*miss_country/n,2) AS pct_missing_country,\n",
        "       ROUND(100*miss_plan/n,2)   AS pct_missing_subscription_plan,\n",
        "       ROUND(100*miss_age/n,2)    AS pct_missing_age\n",
        "FROM base;\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "b3e17cbb",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((0.0, 0.0, 11.93), {'pct_missing_country': 0, 'pct_missing_subscription_plan': 1, 'pct_missing_age': 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d66fac89"
      },
      "source": [
        "**Reflection:** Which columns are most missing? Hypothesize MCAR/MAR/MNAR and why.\n",
        "\n",
        "The **`age`** column is the most (and only) missing column, with **11.93%** of its values missing.\n",
        "\n",
        "Here is a hypothesis about the type of missingness:\n",
        "\n",
        "*   **MCAR (Missing Completely at Random):** This would mean the reason for the missing age data is unrelated to any other user attribute or the age itself. This is unlikely for demographic data but could occur due to random system errors during data collection.\n",
        "\n",
        "*   **MAR (Missing at Random):** This would mean the probability of age being missing is related to another observed variable. For example, users in a specific `country` or on a `Basic` `subscription_plan` might be less likely to provide their age due to differences in the sign-up form or regional privacy norms. This is a plausible scenario.\n",
        "\n",
        "*   **MNAR (Missing Not at Random):** This is the most likely hypothesis. The missingness is related to the value of the `age` column itself. Users at the extreme ends of the age spectrum (e.g., very young or very old) might be more reluctant to disclose their age due to privacy concerns. To confirm this, we would need to find a proxy for age or analyze the data after it has been (if ever) collected.\n",
        "\n",
        "Given the personal nature of age information, **MNAR is the strongest hypothesis**, as the act of withholding one's age is often directly related to the age itself."
      ],
      "id": "d66fac89"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T73XrgnIoASm"
      },
      "source": [
        "### 5.2 Duplicates (watch_history) — What & Why\n",
        "Find exact duplicate interaction records and keep **one best** per group (deterministic policy)."
      ],
      "id": "T73XrgnIoASm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5ZegFSkoASm"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Report duplicate groups on `(user_id, movie_id, event_ts, device_type)` with counts (top 20).\n",
        "2) Create table `watch_history_dedup` that keeps one row per group (prefer higher `progress_ratio`, then `minutes_watched`). Add comments.\n"
      ],
      "id": "I5ZegFSkoASm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH-xK-vcoASm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6491fcae-88ca-4e3e-fa3c-09a632acfd93"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('user_03310', 'movie_0640', datetime.date(2024, 9, 8), 'Smart TV', 16), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_00391', 'movie_0893', datetime.date(2024, 8, 26), 'Laptop', 16), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_09815', 'movie_0827', datetime.date(2024, 5, 25), 'Laptop', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_04899', 'movie_0142', datetime.date(2025, 1, 20), 'Desktop', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_01469', 'movie_0237', datetime.date(2025, 1, 17), 'Laptop', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_02652', 'movie_0352', datetime.date(2024, 10, 22), 'Desktop', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_05952', 'movie_0893', datetime.date(2024, 4, 29), 'Desktop', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_04506', 'movie_0244', datetime.date(2025, 5, 27), 'Desktop', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_02126', 'movie_0642', datetime.date(2025, 2, 9), 'Desktop', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03898', 'movie_0500', datetime.date(2025, 7, 29), 'Desktop', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03660', 'movie_0109', datetime.date(2025, 5, 20), 'Desktop', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03408', 'movie_0146', datetime.date(2025, 6, 2), 'Desktop', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_01807', 'movie_0921', datetime.date(2025, 1, 30), 'Laptop', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_00564', 'movie_0234', datetime.date(2024, 1, 9), 'Laptop', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_05629', 'movie_0697', datetime.date(2025, 1, 23), 'Desktop', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_06799', 'movie_0458', datetime.date(2024, 8, 15), 'Desktop', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_09454', 'movie_0116', datetime.date(2025, 10, 19), 'Laptop', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_01143', 'movie_0166', datetime.date(2024, 5, 28), 'Laptop', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_02028', 'movie_0037', datetime.date(2024, 8, 13), 'Desktop', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_06103', 'movie_0113', datetime.date(2025, 4, 8), 'Laptop', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — Detect duplicate groups (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT user_id, movie_id, watch_date, device_type, COUNT(*) AS dup_count\n",
        "FROM `{project_id}.netflix.watch_history`\n",
        "GROUP BY user_id, movie_id, watch_date, device_type\n",
        "HAVING dup_count > 1\n",
        "ORDER BY dup_count DESC\n",
        "LIMIT 20;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "AH-xK-vcoASm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GsXzPdQoASq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2854f848-394a-4673-c4fd-5c6f09d8b719"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deduplicated table watch_history_dedup created successfully.\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — Keep-one policy (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{project_id}.netflix.watch_history_dedup` AS\n",
        "SELECT * EXCEPT(rk) FROM (\n",
        "  SELECT h.*,\n",
        "         ROW_NUMBER() OVER (\n",
        "           PARTITION BY user_id, movie_id, watch_date, device_type\n",
        "           ORDER BY progress_percentage DESC, watch_duration_minutes DESC\n",
        "         ) AS rk\n",
        "  FROM `{project_id}.netflix.watch_history` h\n",
        ")\n",
        "WHERE rk = 1;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "print(\"Deduplicated table watch_history_dedup created successfully.\")"
      ],
      "id": "_GsXzPdQoASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mrF9O0UoASq"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a before/after count query comparing raw vs `watch_history_dedup`.\n"
      ],
      "id": "6mrF9O0UoASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyNoGyV0oASq"
      },
      "source": [
        "**Reflection:** Why do duplicates arise (natural vs system-generated)? How do they corrupt labels and KPIs?\n",
        "\n",
        "\n",
        "Duplicates fundamentally arise from two main sources: **natural variations** and **system-generated errors**. Natural duplicates, also known as legitimate duplicates, occur when distinct entries genuinely refer to the same real-world entity due to valid, real-world variations, such as different spellings of a name (e.g., \"John Smith\" vs. \"J. Smith\"), the integration of data from multiple unharmonized source systems, or time-based updates that fail to consolidate old and new records (e.g., a customer changing their address). Conversely, system-generated, or artificial, duplicates are technical errors introduced by faulty processes, such as flawed Extract, Transform, Load (ETL) scripts that rerun and insert the same batch multiple times, system bugs that cause transactions to be logged repeatedly, or a failure to enforce unique primary keys. Both types corrupt data integrity, but while natural duplicates require complex matching algorithms to resolve, system-generated duplicates usually point to underlying process failures.\n",
        "\n",
        "The presence of these duplicates severely corrupts both data **labels** and organizational **Key Performance Indicators (KPIs)**. Duplicates introduce significant bias and inaccuracy by creating a false representation of reality. At the level of individual **labels**, duplicates directly inflate counts—for instance, a single customer counted three times makes the total customer base appear artificially larger. This leads to misleading aggregations, where sums (like total revenue) are grossly overstated, and averages (like average order value) are skewed. In the context of machine learning, duplicate records act as synthetic oversampling, biasing training models and causing them to overfit the specific noise of the error. This corruption scales up to affect **KPIs**: metrics like total customer count, market penetration, and conversion rates are often falsely inflated, giving management an overoptimistic and inaccurate view of growth and operational efficiency. Ultimately, corrupted labels and KPIs lead to flawed analytical insights, poor resource allocation, and strategically unsound business decisions."
      ],
      "id": "vyNoGyV0oASq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c029865a",
        "outputId": "1be0a2cd-c618-4eb9-9fa3-7d23b8b059ae"
      },
      "source": [
        "# Verification: Before/after count query comparing raw vs watch_history_dedup\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT 'raw' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.watch_history`\n",
        "UNION ALL\n",
        "SELECT 'deduplicated' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.watch_history_dedup`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "print(\"Row counts before and after deduplication:\")\n",
        "for row in results:\n",
        "    print(f\"Table: {row.table_name:<15} | Rows: {row.row_count}\")"
      ],
      "id": "c029865a",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row counts before and after deduplication:\n",
            "Table: deduplicated    | Rows: 100000\n",
            "Table: raw             | Rows: 420000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFlt_N03oASq"
      },
      "source": [
        "### 5.3 Outliers (minutes_watched) — What & Why\n",
        "Estimate extreme values via IQR; report % outliers; **winsorize** to P01/P99 for robustness while also **flagging** extremes."
      ],
      "id": "YFlt_N03oASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGiM4dQToASq"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Compute IQR bounds for `minutes_watched` on `watch_history_dedup` and report % outliers.\n",
        "2) Create `watch_history_robust` with `minutes_watched_capped` capped at P01/P99; return quantile summaries before/after.\n"
      ],
      "id": "QGiM4dQToASq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "143550e6",
        "outputId": "f4c71f67-365d-4e2a-f5db-1261c8539c5b"
      },
      "source": [
        "# Compute IQR bounds for watch_duration_minutes on watch_history_dedup and report % outliers\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "WITH dist AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(1)] AS q1,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(3)] AS q3\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        "),\n",
        "bounds AS (\n",
        "  SELECT q1, q3, (q3-q1) AS iqr,\n",
        "         q1 - 1.5*(q3-q1) AS lo,\n",
        "         q3 + 1.5*(q3-q1) AS hi\n",
        "  FROM dist\n",
        ")\n",
        "SELECT\n",
        "  COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi) AS outliers,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi)/COUNT(*),2) AS pct_outliers\n",
        "FROM `{project_id}.netflix.watch_history_dedup` h\n",
        "CROSS JOIN bounds b;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "143550e6",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((3433, 100000, 3.43), {'outliers': 0, 'total': 1, 'pct_outliers': 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuY6Qv5UoASq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e9d9758-1d29-4a39-cdeb-d7dc7135c5d4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('after', [4.4, 24.6, 41.5, 61.5, 92.0, 203.6]), {'which': 0, 'q': 1})\n",
            "Row(('before', [0.2, 24.8, 41.9, 61.4, 91.8, 799.3]), {'which': 0, 'q': 1})\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — Winsorize + quantiles (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{project_id}.netflix.watch_history_robust` AS\n",
        "WITH q AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(1)]  AS p01,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(98)] AS p99\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        ")\n",
        "SELECT\n",
        "  h.*,\n",
        "  GREATEST(q.p01, LEAST(q.p99, h.watch_duration_minutes)) AS watch_duration_minutes_capped\n",
        "FROM `{project_id}.netflix.watch_history_dedup` h, q;\n",
        "\n",
        "-- Quantiles before vs after\n",
        "WITH before AS (\n",
        "  SELECT 'before' AS which, APPROX_QUANTILES(watch_duration_minutes, 5) AS q\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        "),\n",
        "after AS (\n",
        "  SELECT 'after' AS which, APPROX_QUANTILES(watch_duration_minutes_capped, 5) AS q\n",
        "  FROM `{project_id}.netflix.watch_history_robust`\n",
        ")\n",
        "SELECT * FROM before UNION ALL SELECT * FROM after;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "OuY6Qv5UoASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYXOx0WioASq"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that shows min/median/max before vs after capping.\n"
      ],
      "id": "iYXOx0WioASq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23379d21",
        "outputId": "7c8e552c-2ed8-4e0f-bd17-acedcf114ba4"
      },
      "source": [
        "# Verification: Show min/median/max before vs after capping\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "WITH before AS (\n",
        "  SELECT\n",
        "    'before' AS which,\n",
        "    MIN(watch_duration_minutes) AS min_val,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 2)[OFFSET(1)] AS median_val,\n",
        "    MAX(watch_duration_minutes) AS max_val\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        "),\n",
        "after AS (\n",
        "  SELECT\n",
        "    'after' AS which,\n",
        "    MIN(watch_duration_minutes_capped) AS min_val,\n",
        "    APPROX_QUANTILES(watch_duration_minutes_capped, 2)[OFFSET(1)] AS median_val,\n",
        "    MAX(watch_duration_minutes_capped) AS max_val\n",
        "  FROM `{project_id}.netflix.watch_history_robust`\n",
        ")\n",
        "SELECT * FROM before UNION ALL SELECT * FROM after;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "23379d21",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('after', 4.4, 51.4, 203.6), {'which': 0, 'min_val': 1, 'median_val': 2, 'max_val': 3})\n",
            "Row(('before', 0.2, 51.0, 799.3), {'which': 0, 'min_val': 1, 'median_val': 2, 'max_val': 3})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gubOakeIoASq"
      },
      "source": [
        "**Reflection:** When might capping be harmful? Name a model type less sensitive to outliers and why.\n",
        "\n",
        "\n",
        "Data capping, or Winsorizing, is a powerful technique for handling outliers, but it becomes actively **harmful** when the extreme values being modified are **genuine, critical data points** rather than simple errors or noise, leading to a serious loss of real business insight by underestimating the impact of rare, high-value events (like a major customer purchase or a sudden system failure) and artificially compressing the data's true variance, which in turn leads to faulty statistical inferences and models that cannot accurately predict worst-case scenarios or peak performance because the high end of the distribution has been manually truncated. The model type that is notably less sensitive to outliers is the **Random Forest** (a type of tree-based ensemble model) because its foundation, the **Decision Tree**, operates using simple splitting rules (e.g., \"Is the value greater than X?\") rather than distance-based metrics, meaning the exact, extreme magnitude of an outlier doesn't disproportionately affect the splitting logic, and furthermore, the final prediction in a Random Forest is often based on the **median** (or a voting process) of all individual trees, making it inherently more **robust** than models like standard Linear Regression, which use the highly outlier-sensitive mean and minimize the squared error, thereby giving squared-up influence to extreme values."
      ],
      "id": "gubOakeIoASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtXNlTftoASq"
      },
      "source": [
        "### 5.4 Business anomaly flags — What & Why\n",
        "Human-readable flags help both product decisioning and ML features (e.g., binge behavior)."
      ],
      "id": "xtXNlTftoASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOJ7CAHkoASq"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **three BigQuery SQL cells** (adjust if columns differ):\n",
        "1) In `watch_history_robust`, compute and summarize `flag_binge` for sessions > 8 hours.\n",
        "2) In `users`, compute and summarize `flag_age_extreme` if age can be parsed from `age_band` (<10 or >100).\n",
        "3) In `movies`, compute and summarize `flag_duration_anomaly` where `duration_min` < 15 or > 480 (if exists).\n",
        "Each cell should output count and percentage and include 1–2 comments.\n"
      ],
      "id": "VOJ7CAHkoASq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qfvn5Vn7oASq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89451e50-8c69-44c0-a9ef-d51c4e16c6cd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((0, 100000, 0.0), {'sessions_over_8h': 0, 'total': 1, 'pct': 2})\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — flag_binge (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(watch_duration_minutes_capped > 8*60) AS sessions_over_8h,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(watch_duration_minutes_capped > 8*60)/COUNT(*),2) AS pct\n",
        "FROM `{project_id}.netflix.watch_history_robust`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "Qfvn5Vn7oASq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CglM1uhBoASr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd64faac-fefc-428a-b865-9a159be9c362"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((716, 41200, 1.74), {'extreme_age_rows': 0, 'total': 1, 'pct': 2})\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — flag_age_extreme (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(age < 10 OR age > 100) AS extreme_age_rows,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(age < 10 OR age > 100)/COUNT(*),2) AS pct\n",
        "FROM `{project_id}.netflix.users`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "CglM1uhBoASr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kho40L1oASr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7181fd98-b012-4a56-efc5-958f192fda0f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((48, 44, 4160, 2.21), {'titles_under_15m': 0, 'titles_over_8h': 1, 'total': 2, 'pct_duration_anomaly': 3})\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — flag_duration_anomaly (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(duration_minutes < 15) AS titles_under_15m,\n",
        "  COUNTIF(duration_minutes > 480) AS titles_over_8h,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(duration_minutes < 15 OR duration_minutes > 480)/COUNT(*),2) AS pct_duration_anomaly\n",
        "FROM `{project_id}.netflix.movies`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "2kho40L1oASr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yhdFbkaoASr"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single compact summary query that returns two columns per flag: `flag_name, pct_of_rows`.\n"
      ],
      "id": "5yhdFbkaoASr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZK1fkr4oASr"
      },
      "source": [
        "**Reflection:** Which anomaly flag is most common? Which would you keep as a feature and why?\n",
        "\n",
        "flag_duration_anamoly is the most common. However, when building a machine learning model to predict user behavior (like what they'll watch next or if they will cancel their subscription), the goal is to understand the user. Therefore, features that directly describe the user are often the most powerful.\n",
        "\n",
        "Here’s a comparison:\n",
        "\n",
        "flag_age_extreme: This is a direct attribute of the user. It tells us that this person belongs to a specific demographic group (very young or elderly). This is a strong signal because different age groups have vastly different viewing habits, content preferences, and churn risks. A model can learn patterns like \"users with flag_age_extreme = true are highly likely to watch animated content.\"\n",
        "\n",
        "flag_duration_anomaly: This flag describes the movie, not the user. It tells us a movie is unusually short or long. While a user chooses to watch it, the flag itself is a property of the content. It's less predictive of the user's overall behavior. Knowing a user watched one weirdly long movie doesn't tell us as much about them as knowing their age.\n",
        "\n",
        "In short, we chose flag_age_extreme because it helps create a clearer profile of the user, which is exactly what a user-centric prediction model needs."
      ],
      "id": "pZK1fkr4oASr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c9573e0",
        "outputId": "a9be19b1-fa44-4611-d295-c7738a38736b"
      },
      "source": [
        "# Verification: Single compact summary query for anomaly flags\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  'flag_binge' AS flag_name,\n",
        "  ROUND(100*COUNTIF(watch_duration_minutes_capped > 8*60)/COUNT(*),2) AS pct_of_rows\n",
        "FROM `{project_id}.netflix.watch_history_robust`\n",
        "UNION ALL\n",
        "SELECT\n",
        "  'flag_age_extreme' AS flag_name,\n",
        "  ROUND(100*COUNTIF(age < 10 OR age > 100)/COUNT(*),2) AS pct_of_rows\n",
        "FROM `{project_id}.netflix.users`\n",
        "UNION ALL\n",
        "SELECT\n",
        "  'flag_duration_anomaly' AS flag_name,\n",
        "  ROUND(100*COUNTIF(duration_minutes < 15 OR duration_minutes > 480)/COUNT(*),2) AS pct_of_rows\n",
        "FROM `{project_id}.netflix.movies`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "8c9573e0",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('flag_binge', 0.0), {'flag_name': 0, 'pct_of_rows': 1})\n",
            "Row(('flag_age_extreme', 1.74), {'flag_name': 0, 'pct_of_rows': 1})\n",
            "Row(('flag_duration_anomaly', 2.21), {'flag_name': 0, 'pct_of_rows': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4011b829",
        "outputId": "15cc342a-d09b-4b9c-9a46-3108579e84ea"
      },
      "source": [
        "# In movies, compute and summarize flag_duration_anomaly where duration_minutes < 15 or > 480.\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(duration_minutes < 15) AS titles_under_15m,\n",
        "  COUNTIF(duration_minutes > 480) AS titles_over_8h,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(duration_minutes < 15 OR duration_minutes > 480)/COUNT(*),2) AS pct_duration_anomaly\n",
        "FROM `{project_id}.netflix.movies`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "4011b829",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((48, 44, 4160, 2.21), {'titles_under_15m': 0, 'titles_over_8h': 1, 'total': 2, 'pct_duration_anomaly': 3})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yKDRvHtoASr"
      },
      "source": [
        "## 6) Save & submit — What & Why\n",
        "Reproducibility: save artifacts and document decisions so others can rerun and audit."
      ],
      "id": "8yKDRvHtoASr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMy5HEutoASr"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a checklist (Markdown) students can paste at the end:\n",
        "- Save this notebook to the team Drive.\n",
        "- Export a `.sql` file with your DQ queries and save to repo.\n",
        "- Push notebook + SQL to the **team GitHub** with a descriptive commit.\n",
        "- Add a README with your `PROJECT_ID`, `REGION`, bucket, dataset, and today’s row counts.\n"
      ],
      "id": "PMy5HEutoASr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5abacc8b"
      },
      "source": [
        "### Save & submit checklist:\n",
        "\n",
        "- [ ] Save this notebook to the team Drive.\n",
        "- [ ] Export a `.sql` file with your DQ queries and save to repo.\n",
        "- [ ] Push notebook + SQL to the **team GitHub** with a descriptive commit.\n",
        "- [ ] Add a README with your `PROJECT_ID`, `REGION`, bucket, dataset, and today’s row counts."
      ],
      "id": "5abacc8b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy3qqUtyoASr"
      },
      "source": [
        "## Grading rubric (quick)\n",
        "- Profiling completeness (30)  \n",
        "- Cleaning policy correctness & reproducibility (40)  \n",
        "- Reflection/insight (20)  \n",
        "- Hygiene (naming, verification, idempotence) (10)\n"
      ],
      "id": "Iy3qqUtyoASr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df955b23"
      },
      "source": [
        "# Task\n",
        "Create a project summary markdown cell listing the project ID, region, bucket name, and BigQuery dataset. Then, generate a Python cell that exports all Data Quality (DQ) queries for missingness, duplicates, outliers, and anomaly flags into a single SQL file named \"dq_queries.sql\"."
      ],
      "id": "df955b23"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3c369e9"
      },
      "source": [
        "## Create Project Summary Cell\n",
        "\n",
        "### Subtask:\n",
        "Generate a new markdown cell that lists the current PROJECT_ID, REGION, BUCKET_NAME, and the BigQuery dataset name for documentation purposes.\n"
      ],
      "id": "a3c369e9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6453415"
      },
      "source": [
        "**Reasoning**:\n",
        "To dynamically create the project summary with live values from the notebook's environment, I will generate a Python code cell. This cell will retrieve the `PROJECT_ID`, `REGION`, and `BUCKET_NAME` variables and then use `IPython.display.Markdown` to render them in a clean, formatted list as requested. This approach correctly uses f-strings as specified and ensures the summary is accurate and reproducible.\n",
        "\n"
      ],
      "id": "f6453415"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "54392940",
        "outputId": "a24ae3b2-e1c2-43ec-80bf-f03624021dd3"
      },
      "source": [
        "import os\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Retrieve the project details from environment variables and Python variables\n",
        "project_id = os.environ.get('GOOGLE_CLOUD_PROJECT', 'PROJECT_ID_NOT_FOUND')\n",
        "bucket_name = os.environ.get('BUCKET_NAME', 'BUCKET_NAME_NOT_FOUND')\n",
        "try:\n",
        "    region = REGION\n",
        "except NameError:\n",
        "    region = 'us-central1' # Fallback to the value set in the notebook\n",
        "dataset = 'netflix'\n",
        "\n",
        "# Create the markdown content\n",
        "summary_markdown = f\"\"\"\n",
        "## Project Summary\n",
        "\n",
        "*   **Project ID:** `{project_id}`\n",
        "*   **Region:** `{region}`\n",
        "*   **GCS Bucket:** `{bucket_name}`\n",
        "*   **BigQuery Dataset:** `{dataset}`\n",
        "\"\"\"\n",
        "\n",
        "# Display the content as a rendered markdown cell\n",
        "display(Markdown(summary_markdown))"
      ],
      "id": "54392940",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Project Summary\n\n*   **Project ID:** `noble-broker-471012-q6`\n*   **Region:** `us-central1`\n*   **GCS Bucket:** `mgmt467-netflix-3fc7b4d1`\n*   **BigQuery Dataset:** `netflix`\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1980dcab"
      },
      "source": [
        "## Export DQ Queries to SQL File\n",
        "\n",
        "### Subtask:\n",
        "Generate a Python code cell that gathers all the Data Quality (DQ) queries from the notebook (for missingness, duplicates, outliers, and anomaly flags) and saves them into a single file named `dq_queries.sql`.\n"
      ],
      "id": "1980dcab"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10531eac"
      },
      "source": [
        "**Reasoning**:\n",
        "I will now generate the Python code to collect all the previously used Data Quality queries into a single string, write that string to a `dq_queries.sql` file, and then print the file's contents to verify its creation, as per the detailed instructions.\n",
        "\n"
      ],
      "id": "10531eac"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca6eacaa"
      },
      "source": [
        "**Reasoning**:\n",
        "I will re-generate the Python code cell that combines all the specified Data Quality (DQ) queries into a single f-string, writes this string to a file named `dq_queries.sql`, and then verifies the file's contents by printing them. This directly follows the user's detailed instructions.\n",
        "\n"
      ],
      "id": "ca6eacaa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6f7198e",
        "outputId": "479a0ae3-9375-4f04-ee81-f38f5b87e891"
      },
      "source": [
        "import os\n",
        "\n",
        "# Get project ID from environment variables to make queries runnable\n",
        "project_id = os.environ.get('GOOGLE_CLOUD_PROJECT', 'your-gcp-project-id')\n",
        "dataset = 'netflix'\n",
        "\n",
        "# Combine all Data Quality (DQ) queries into a single f-string\n",
        "all_dq_queries = f\"\"\"\n",
        "-- ====================================================================\n",
        "-- MGMT 467: Data Quality (DQ) Queries for Netflix Dataset\n",
        "-- ====================================================================\n",
        "\n",
        "-- 5.1 Missingness: Profile missing values in the users table\n",
        "-- This query calculates the total count and percentage of missing values\n",
        "-- for key demographic and subscription columns.\n",
        "WITH base_missingness AS (\n",
        "  SELECT\n",
        "    COUNT(*) AS n,\n",
        "    COUNTIF(country IS NULL) AS miss_country,\n",
        "    COUNTIF(subscription_plan IS NULL) AS miss_plan,\n",
        "    COUNTIF(age IS NULL) AS miss_age\n",
        "  FROM `{project_id}.{dataset}.users`\n",
        ")\n",
        "SELECT\n",
        "  n AS total_rows,\n",
        "  ROUND(100 * miss_country / n, 2) AS pct_missing_country,\n",
        "  ROUND(100 * miss_plan / n, 2) AS pct_missing_subscription_plan,\n",
        "  ROUND(100 * miss_age / n, 2) AS pct_missing_age\n",
        "FROM base_missingness;\n",
        "\n",
        "-- 5.2 Duplicates: Detect and remove duplicate watch history records\n",
        "-- This query identifies duplicate records based on a composite key.\n",
        "SELECT\n",
        "  user_id, movie_id, watch_date, device_type, COUNT(*) AS dup_count\n",
        "FROM `{project_id}.{dataset}.watch_history`\n",
        "GROUP BY 1, 2, 3, 4\n",
        "HAVING dup_count > 1\n",
        "ORDER BY dup_count DESC\n",
        "LIMIT 20;\n",
        "\n",
        "-- This query creates a new table with duplicates removed, keeping the most complete record.\n",
        "CREATE OR REPLACE TABLE `{project_id}.{dataset}.watch_history_dedup` AS\n",
        "SELECT * EXCEPT(rk) FROM (\n",
        "  SELECT\n",
        "    h.*,\n",
        "    ROW_NUMBER() OVER (\n",
        "      PARTITION BY user_id, movie_id, watch_date, device_type\n",
        "      ORDER BY progress_percentage DESC, watch_duration_minutes DESC\n",
        "    ) AS rk\n",
        "  FROM `{project_id}.{dataset}.watch_history` h\n",
        ")\n",
        "WHERE rk = 1;\n",
        "\n",
        "-- 5.3 Outliers: Identify and cap extreme values in watch duration\n",
        "-- This query calculates outliers based on the 1.5*IQR rule.\n",
        "WITH dist AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(1)] AS q1,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(3)] AS q3\n",
        "  FROM `{project_id}.{dataset}.watch_history_dedup`\n",
        "),\n",
        "bounds AS (\n",
        "  SELECT\n",
        "    (q3 - q1) AS iqr,\n",
        "    q1 - 1.5 * (q3 - q1) AS lo,\n",
        "    q3 + 1.5 * (q3 - q1) AS hi\n",
        "  FROM dist\n",
        ")\n",
        "SELECT\n",
        "  COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi) AS outliers,\n",
        "  ROUND(100 * COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi) / COUNT(*), 2) AS pct_outliers\n",
        "FROM `{project_id}.{dataset}.watch_history_dedup` h CROSS JOIN bounds b;\n",
        "\n",
        "-- This query creates a robust table by capping (Winsorizing) outliers at the 1st and 99th percentiles.\n",
        "CREATE OR REPLACE TABLE `{project_id}.{dataset}.watch_history_robust` AS\n",
        "WITH q AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(1)] AS p01,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(99)] AS p99\n",
        "  FROM `{project_id}.{dataset}.watch_history_dedup`\n",
        ")\n",
        "SELECT\n",
        "  h.*,\n",
        "  GREATEST(q.p01, LEAST(q.p99, h.watch_duration_minutes)) AS watch_duration_minutes_capped\n",
        "FROM `{project_id}.{dataset}.watch_history_dedup` h, q;\n",
        "\n",
        "-- 5.4 Business Anomaly Flags\n",
        "-- Flag 1: Binge watching sessions (> 8 hours)\n",
        "SELECT\n",
        "  COUNTIF(watch_duration_minutes_capped > 8 * 60) AS sessions_over_8h,\n",
        "  ROUND(100 * COUNTIF(watch_duration_minutes_capped > 8 * 60) / COUNT(*), 2) AS pct\n",
        "FROM `{project_id}.{dataset}.watch_history_robust`;\n",
        "\n",
        "-- Flag 2: Extreme user ages (< 10 or > 100)\n",
        "SELECT\n",
        "  COUNTIF(age < 10 OR age > 100) AS extreme_age_rows,\n",
        "  ROUND(100 * COUNTIF(age < 10 OR age > 100) / COUNT(*), 2) AS pct\n",
        "FROM `{project_id}.{dataset}.users`;\n",
        "\n",
        "-- Flag 3: Anomalous movie durations (< 15 min or > 8 hours)\n",
        "SELECT\n",
        "  COUNTIF(duration_minutes < 15 OR duration_minutes > 480) AS anomaly_duration_titles,\n",
        "  ROUND(100 * COUNTIF(duration_minutes < 15 OR duration_minutes > 480) / COUNT(*), 2) AS pct_duration_anomaly\n",
        "FROM `{project_id}.{dataset}.movies`;\n",
        "\"\"\"\n",
        "\n",
        "# Write the queries to a .sql file\n",
        "file_name = 'dq_queries.sql'\n",
        "with open(file_name, 'w') as f:\n",
        "    f.write(all_dq_queries)\n",
        "\n",
        "print(f\"Successfully wrote all DQ queries to {file_name}.\\n\")\n",
        "\n",
        "# Read the file back and print its contents for verification\n",
        "print(\"--- Contents of dq_queries.sql ---\")\n",
        "with open(file_name, 'r') as f:\n",
        "    print(f.read())"
      ],
      "id": "f6f7198e",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully wrote all DQ queries to dq_queries.sql.\n",
            "\n",
            "--- Contents of dq_queries.sql ---\n",
            "\n",
            "-- ====================================================================\n",
            "-- MGMT 467: Data Quality (DQ) Queries for Netflix Dataset\n",
            "-- ====================================================================\n",
            "\n",
            "-- 5.1 Missingness: Profile missing values in the users table\n",
            "-- This query calculates the total count and percentage of missing values\n",
            "-- for key demographic and subscription columns.\n",
            "WITH base_missingness AS (\n",
            "  SELECT\n",
            "    COUNT(*) AS n,\n",
            "    COUNTIF(country IS NULL) AS miss_country,\n",
            "    COUNTIF(subscription_plan IS NULL) AS miss_plan,\n",
            "    COUNTIF(age IS NULL) AS miss_age\n",
            "  FROM `noble-broker-471012-q6.netflix.users`\n",
            ")\n",
            "SELECT\n",
            "  n AS total_rows,\n",
            "  ROUND(100 * miss_country / n, 2) AS pct_missing_country,\n",
            "  ROUND(100 * miss_plan / n, 2) AS pct_missing_subscription_plan,\n",
            "  ROUND(100 * miss_age / n, 2) AS pct_missing_age\n",
            "FROM base_missingness;\n",
            "\n",
            "-- 5.2 Duplicates: Detect and remove duplicate watch history records\n",
            "-- This query identifies duplicate records based on a composite key.\n",
            "SELECT\n",
            "  user_id, movie_id, watch_date, device_type, COUNT(*) AS dup_count\n",
            "FROM `noble-broker-471012-q6.netflix.watch_history`\n",
            "GROUP BY 1, 2, 3, 4\n",
            "HAVING dup_count > 1\n",
            "ORDER BY dup_count DESC\n",
            "LIMIT 20;\n",
            "\n",
            "-- This query creates a new table with duplicates removed, keeping the most complete record.\n",
            "CREATE OR REPLACE TABLE `noble-broker-471012-q6.netflix.watch_history_dedup` AS\n",
            "SELECT * EXCEPT(rk) FROM (\n",
            "  SELECT\n",
            "    h.*,\n",
            "    ROW_NUMBER() OVER (\n",
            "      PARTITION BY user_id, movie_id, watch_date, device_type\n",
            "      ORDER BY progress_percentage DESC, watch_duration_minutes DESC\n",
            "    ) AS rk\n",
            "  FROM `noble-broker-471012-q6.netflix.watch_history` h\n",
            ")\n",
            "WHERE rk = 1;\n",
            "\n",
            "-- 5.3 Outliers: Identify and cap extreme values in watch duration\n",
            "-- This query calculates outliers based on the 1.5*IQR rule.\n",
            "WITH dist AS (\n",
            "  SELECT\n",
            "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(1)] AS q1,\n",
            "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(3)] AS q3\n",
            "  FROM `noble-broker-471012-q6.netflix.watch_history_dedup`\n",
            "),\n",
            "bounds AS (\n",
            "  SELECT\n",
            "    (q3 - q1) AS iqr,\n",
            "    q1 - 1.5 * (q3 - q1) AS lo,\n",
            "    q3 + 1.5 * (q3 - q1) AS hi\n",
            "  FROM dist\n",
            ")\n",
            "SELECT\n",
            "  COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi) AS outliers,\n",
            "  ROUND(100 * COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi) / COUNT(*), 2) AS pct_outliers\n",
            "FROM `noble-broker-471012-q6.netflix.watch_history_dedup` h CROSS JOIN bounds b;\n",
            "\n",
            "-- This query creates a robust table by capping (Winsorizing) outliers at the 1st and 99th percentiles.\n",
            "CREATE OR REPLACE TABLE `noble-broker-471012-q6.netflix.watch_history_robust` AS\n",
            "WITH q AS (\n",
            "  SELECT\n",
            "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(1)] AS p01,\n",
            "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(99)] AS p99\n",
            "  FROM `noble-broker-471012-q6.netflix.watch_history_dedup`\n",
            ")\n",
            "SELECT\n",
            "  h.*,\n",
            "  GREATEST(q.p01, LEAST(q.p99, h.watch_duration_minutes)) AS watch_duration_minutes_capped\n",
            "FROM `noble-broker-471012-q6.netflix.watch_history_dedup` h, q;\n",
            "\n",
            "-- 5.4 Business Anomaly Flags\n",
            "-- Flag 1: Binge watching sessions (> 8 hours)\n",
            "SELECT\n",
            "  COUNTIF(watch_duration_minutes_capped > 8 * 60) AS sessions_over_8h,\n",
            "  ROUND(100 * COUNTIF(watch_duration_minutes_capped > 8 * 60) / COUNT(*), 2) AS pct\n",
            "FROM `noble-broker-471012-q6.netflix.watch_history_robust`;\n",
            "\n",
            "-- Flag 2: Extreme user ages (< 10 or > 100)\n",
            "SELECT\n",
            "  COUNTIF(age < 10 OR age > 100) AS extreme_age_rows,\n",
            "  ROUND(100 * COUNTIF(age < 10 OR age > 100) / COUNT(*), 2) AS pct\n",
            "FROM `noble-broker-471012-q6.netflix.users`;\n",
            "\n",
            "-- Flag 3: Anomalous movie durations (< 15 min or > 8 hours)\n",
            "SELECT\n",
            "  COUNTIF(duration_minutes < 15 OR duration_minutes > 480) AS anomaly_duration_titles,\n",
            "  ROUND(100 * COUNTIF(duration_minutes < 15 OR duration_minutes > 480) / COUNT(*), 2) AS pct_duration_anomaly\n",
            "FROM `noble-broker-471012-q6.netflix.movies`;\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "name": "MGMT467_PromptPlusExamples_Colab_Kaggle_GCS_BQ_DQ_prof.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}